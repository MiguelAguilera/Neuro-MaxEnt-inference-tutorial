{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KyeMKUIGgzC5SXDgpC9xJpM1Vckdcq-b",
      "authorship_tag": "ABX9TyP4uEp7gLOz/VAr0Xes6I+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelAguilera/Neuro-MaxEnt-inference-tutorial/blob/main/Spike_train_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "Most of the interesting things that happen in living organisms result from networks of interactions, whether among neurons in the brain, genes in a single cell, or amino acids in single protein molecule.  Especially in the context of neural networks, there is a long tradition of using ideas from statistical physics to think about the emergence of collective behavior from the microscopic interactions, with the hope that this functional collective behavior will be robust (universal?) to our ignorance of many details in these systems.  In the past decade or so, new experimental techniques have made it possible to monitor the activity of many biological networks much more completely, and the availability of these data has made the problems of analysis much more urgent: given what the new techniques can measure, can we extract a global picture of the network dynamics?  In this lecture I'll show how an old idea, the maximum entropy construction, can be used to attack this problem.  What is most exciting is that this construction provides a path directly from real data to the classical models of statistical mechanics.  I'll describe in detail how this works for a network of neurons in the retina as it responds to complex, naturalistic inputs, where the relevant model is exactly the Ising model with pairwise, frustrated interactions.   Remarkably, the data suggest that the system is poised very close to a critical point. I'll try to highlight some open theoretical questions in this field, as well as making connections to other systems.  Again, I hope we'll see the outlines of how common theoretical ideas can unify our understanding of diverse systems.  \n",
        "\n",
        "Physicists have long explored analogies between the\n",
        "statistical mechanics of Ising models and the functional\n",
        "dynamics of neural networks [1, 2]. Recently it has been\n",
        "suggested that this analogy can be turned into a precise\n",
        "mapping [3]: In small windows of time, a single neuron i\n",
        "either does ($\\sigma_i = +1$) or does not ($\\sigma_i = -1$) generate an\n",
        "action potential or \"spike\" [4]; if we measure the mean\n",
        "probability of spiking for each cell ($\\langle \\sigma_i\\rangle$) and the correlations between pairs of cells ($C_{ij} = \\langle \\sigma_i\\rangle -\\langle \\sigma_i\\rangle \\langle \\sigma_j\\rangle$), then\n",
        "the maximum entropy model consistent with these data\n",
        "is exactly the Ising model\n",
        "\n",
        "$$ p(\\boldsymbol{ \\sigma} ) = \\frac{1}{Z}\\exp \\left[  \\sum_i^N H_i \\sigma_i + \\sum_{i< j} J_{ij} \\sigma_i \\sigma_j \\right]$$\n",
        "\n",
        "\n",
        "\n",
        "$$ \\mathcal{L} = \\frac{1}{M}\\sum_{m=1}^M \\log p(\\boldsymbol\\sigma^m)$$ \n",
        "\n",
        "$$ \\frac{\\partial\\mathcal{L}}{\\partial H_i} = \\frac{1}{M}\\sum_{m=1}^M  \\sigma_i  - \\langle \\sigma_i  \\rangle  =  \\langle \\sigma_i \\rangle_\\mathrm{data}  -  \\langle \\sigma_i\\rangle_\\mathrm{model} $$ \n",
        "\n",
        "$$ \\frac{\\partial\\mathcal{L}}{\\partial J_{ij}} = \\frac{1}{M}\\sum_{m=1}^M  \\sigma_i \\sigma_j - \\langle \\sigma_i \\sigma_j \\rangle  =  \\langle \\sigma_i \\sigma_j \\rangle_\\mathrm{data}  -  \\langle \\sigma_i \\sigma_j \\rangle_\\mathrm{model} $$ "
      ],
      "metadata": {
        "id": "3Q2PWbYNBMP1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrJEcDOb0usm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "# !pip install requests\n",
        "import requests\n",
        "# Save datagenerators as file to colab working directory\n",
        "# If you are using GitHub, make sure you get the \"Raw\" version of the code\n",
        "url = 'https://raw.githubusercontent.com/MiguelAguilera/Neuro-MaxEnt-inference-tutorial/main/Ising.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "# make sure your filename is the same as how you want to import \n",
        "with open('Ising.py', 'w') as f:\n",
        "    f.write(r.text)\n",
        "\n",
        "# now we can import\n",
        "from Ising import Ising\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating reference data distribution\n",
        "\n"
      ],
      "metadata": {
        "id": "Df5bxS31Posi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N=10\n",
        "T=1000\n",
        "\n",
        "iu1=np.triu_indices(N,1)\n",
        "\n",
        "I_data = Ising(10)\n",
        "I_data.H = -2+ 0.05*np.random.randn(N)\n",
        "I_data.J[iu1] = 1/N + np.random.randn(N*(N-1)//2)/np.sqrt(N)\n",
        "\n",
        "Data_sample = np.zeros((N,T))\n",
        "\n",
        "I_data.randomize_state()\n",
        "for t in range(T):\n",
        "  I_data.SequentialGlauberStep()\n",
        "  Data_sample[:,t] = I_data.s\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20,2))\n",
        "\n",
        "plt.imshow(Data_sample, aspect='auto', interpolation='none')\n",
        "plt.colorbar()\n",
        "\n",
        "ax.set_xlim([0, T])\n",
        "ax.set_xlabel('Time (ms)')\n",
        "# specify tick marks and label label y axis\n",
        "ax.set_yticks(range(N))\n",
        "ax.set_ylabel('Trial Number')\n",
        "ax.set_title('Neuronal Spike Times') \n",
        "\n",
        "m_data = np.mean(Data_sample,axis=1)\n",
        "Cov_data = np.cov(Data_sample)\n",
        "print(m_data)\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(122)\n",
        "plt.imshow(m_data[:,np.newaxis])\n",
        "plt.colorbar()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.imshow(Cov_data)\n",
        "plt.colorbar()\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(I_data.J)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "oJtsa2fn01ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning our model"
      ],
      "metadata": {
        "id": "qwpKIz1_Pvqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N=10\n",
        "T=1000\n",
        "\n",
        "I = Ising(10)\n",
        "I.H = np.zeros(N)\n",
        "I.J = np.zeros((N,N))\n",
        "\n",
        "R=100\n",
        "eta=0.1\n",
        "\n",
        "error = np.zeros(R)\n",
        "\n",
        "for rep in range(R):\n",
        "  Model_sample = np.zeros((N,T))\n",
        "  I.randomize_state()\n",
        "  for t in range(T):\n",
        "    I.SequentialGlauberStep()\n",
        "    Model_sample[:,t] = I.s\n",
        "  m = np.mean(Model_sample,axis=1)\n",
        "  Cov  = np.cov(Model_sample)\n",
        "\n",
        "  I.H += eta*(m_data-m)\n",
        "  I.J[iu1] += eta*(Cov_data-Cov)[iu1]\n",
        "\n",
        "  error[rep] = np.mean((Cov_data-Cov)**2)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(error)\n",
        "\n",
        "plt.figure(figsize=(20,2))\n",
        "plt.imshow(Data_sample, aspect='auto', interpolation='none')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.figure(figsize=(20,2))\n",
        "plt.imshow(Model_sample, aspect='auto', interpolation='none')\n",
        "plt.colorbar()\n",
        "\n",
        "print(m_data)\n",
        "plt.figure()\n",
        "plt.subplot(122)\n",
        "plt.imshow(m_data[:,np.newaxis])\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.imshow(Cov_data)\n",
        "plt.colorbar()\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(I.J)\n",
        "plt.colorbar()\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(121)\n",
        "plt.imshow(Cov_data)\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(Cov)\n",
        "plt.colorbar()\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(121)\n",
        "plt.imshow(I_data.J)\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(I.J)\n",
        "plt.colorbar()\n"
      ],
      "metadata": {
        "id": "wgRVrhJAPxa_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}