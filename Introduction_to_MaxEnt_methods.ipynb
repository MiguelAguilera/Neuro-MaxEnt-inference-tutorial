{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4IIY2URXdsvnl/MO/xlia",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelAguilera/Neuro-MaxEnt-inference-tutorial/blob/main/Introduction_to_MaxEnt_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Most patterns in biology arise from aggregation of many small processes. Variations in the dynamics of complex neural and biochemical networks depend on\n",
        "numerous fluctuations in connectivity and flow through small-scale subcomponents of the network. Variations in cancer onset arise from variable failures in the many individual checks and balances on DNA repair, cell cycle control, and tissue homeostasis. Variations in the ecologicalult value implied by bias is overridden. Note that ddof=1 will return the unbiased estimate, even if both fweights and aweights are specified, and ddof=0 will return the simple av distribution of species follow the myriad local differences in the birth and death rates of species and in the small-scale interactions between particular species.\n",
        "In all such complex systems, we wish to understand how large-scale pattern\n",
        "arises from the aggregation of small-scale processes. A single dominant principle sets the major axis from which all explanation of aggregation and scale must be developed. This dominant principle is the limiting distribution.\n",
        "\n",
        "### A new kind of prior information\n",
        "\n",
        "Imagine a class of problems in which our prior information consists of average\n",
        "values of certain things. What is the less biased model?\n",
        "\n",
        "The notion of ‘entropy’ as originated in thermodynamics is usually associated to that of ‘disorder’ by saying that the former can be regarded as a measure of the latter. The word ‘disorder’ here essentially means ‘randomness’, ‘absence of patterns’, or something similar. While not incorrect, these words clearly require a more precise specification to be useful at a quantitative level. \n",
        "\n",
        "$$ S = - \\sum_{\\mathbf x} p_{\\mathbf x} \\log p_{\\mathbf x}$$\n",
        "\n",
        "We have a total amount of probability\n",
        "$$ \\sum_{\\mathbf x} p_{\\mathbf x}= 1$$\n",
        "\n",
        "\n",
        "> ![Google's logo](https://github.com/MiguelAguilera/Neuro-MaxEnt-inference-tutorial/blob/main/img/entropy.png?raw=true)\n",
        ">\n",
        "> Figure 1. \n",
        "\n",
        "The principle of maximum entropy can be expressed (see [Wikipedia](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy)) as:\n",
        "\n",
        "> The principle of maximum entropy states that, subject to precisely stated prior data (such as a proposition that expresses testable information), the probability distribution which best represents the current state of knowledge is the one with largest entropy. Another way of stating this: Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. According to this principle, the distribution with maximal information entropy is the proper one. … In ordinary language, the principle of maximum entropy can be said to express a claim of epistemic modesty, or of maximum ignorance. The selected distribution is the one that makes the least claim to being informed beyond the stated prior data, that is to say the one that admits the most ignorance beyond the stated prior data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hhVIZjR1ykUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Lagrangian multiplier techinque\n",
        "\n",
        "The maximum entropy principle is a means of deriving probability distributions given certain constraints and the assumption of maximizing entropy. One technique for solving this maximization problem is the Lagrange multiplier technique.\n",
        "\n",
        "Given a multivariable function $f(\\mathbf x, \\lambda)$ and constraints of the form $g(\\mathbf x)=c$, where $g$ is another multivariable function with the same input space as $f$ and $c$ is a constant:\n",
        "\n",
        "In order to minimize (or maximize) the function $f$ consider the following steps, assuming $f$ to be $f(x)$:\n",
        "\n",
        "1. Introduce a new variable $\\lambda$, called Lagrange multiplier, and define a new function $\\mathcal{L}$  with the form:\n",
        "\n",
        "$$ \\mathcal{L}(x,\\lambda) =f(x)+\\lambda(g(x)−c)$$\n",
        "\n",
        "2. Set the derivative of the function  $\\mathcal{L}$  equal to zero:\n",
        "\n",
        "$$ \\frac{\\partial L (x,\\lambda)}{\\partial x_i}=0, \\,\\,\\,\\forall i. \\qquad \\frac{\\partial L (x,\\lambda)}{\\partial \\lambda}=0$$\n",
        "\n",
        "in order to find the critical points of  $\\mathcal{L}$.\n",
        "\n",
        "3. Consider each resulting solution within the limits of the made constraints and derive the resulting distribution $f$, which gives the minimum (or maximum) one is searching for.\n",
        "\n",
        "### Application to the MaxEnt principle\n",
        "\n",
        "Applied to the Maximum Entropy principle, the Lagrangian multiplier technique results in \n",
        "\n",
        "$$ \\max_{p_{\\mathbf x}} \\sum_{\\mathbf x} p_{\\mathbf x} \\log p_{\\mathbf x}$$\n",
        "$$ \\mathrm{s.t.} \\qquad \\sum_{\\mathbf x}  p_{\\mathbf x} f_a(\\mathbf x) = c_a , \\qquad \\sum \\sum_{\\mathbf x} p_{\\mathbf x} =1$$\n",
        "\n",
        "\n",
        "\n",
        "### Example\n",
        "\n",
        "$$ \\mathcal{L} =  -\\sum_{\\mathbf x} p_{\\mathbf x} \\log p_{\\mathbf x}  - \\varphi (\\sum_{\\mathbf x}p_{\\mathbf x} - 1) + \\beta \\sum_a \\theta_a(\\sum_{\\mathbf x} p_{\\mathbf x} f_{i,a}-c_a)$$\n",
        "\n",
        "$$  \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}p_i} =  - 1 - \\log p_{\\mathbf x}\n",
        "- \\varphi  +  \\beta \\sum_a \\theta_a f_{\\mathbf x,a}=0 $$\n",
        "\n",
        "$$ p_{\\mathbf x} \\propto \\exp\\left[\\beta \\sum_a \\theta_a f_{\\mathbf x,a} - \\varphi \\right] $$\n",
        "\n",
        "\n",
        "$$ p_{\\mathbf x} \\propto \\frac{1}{Z}\\exp\\left[\\beta \\sum_a \\theta_a f_{\\mathbf x,a} \\right] $$"
      ],
      "metadata": {
        "id": "vqRimQ7xGQz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### References\n",
        "\n",
        "J. Harte, Maximum Entropy and Ecology: A Theory of Abundance, Distribution, and Energetics. Oxford University Press, 2011\n",
        "\n",
        "E. Montrell, On the entropy function in sociotechnical systems, PNAS, vol. 78 no. 12, 1981\n",
        "\n",
        "S. Frank, The common patterns of nature\n",
        "\n",
        "Academy, Khan. 2019. “Lagrange multipliers, introduction.” 2019. https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint."
      ],
      "metadata": {
        "id": "056AazBwGNfm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8splVbnyjyd"
      },
      "outputs": [],
      "source": []
    }
  ]
}